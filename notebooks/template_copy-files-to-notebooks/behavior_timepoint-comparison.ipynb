{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior Analysis: Comparison of Timepoints"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e03de428c1b3db0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Press SHIFT + ENTER to run code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfd6a360bf17b7d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### USER INPUT!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffb7624fdfd93586"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1ca9fb2a33975a1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-11T18:05:12.304317Z",
     "start_time": "2025-01-11T18:05:12.300300Z"
    }
   },
   "outputs": [],
   "source": [
    "## Define project\n",
    "project_name = ''\n",
    "\n",
    "selected_groups = []\n",
    "selected_conditions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Main Code: Create Individual CSVs for Timepoint Comparisons (Fraction Time in Behavior; Bouts/Min; Mean Bout Duration) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6e3371ebb588a7a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "### Continue to Press SHIFT + ENTER to run code ###\n",
    "\n",
    "# Prompt user for input\n",
    "num_timepoints = int(input(\"Enter the number of time ranges you want to compare (e.g., 2, 3, etc.): \"))\n",
    "time_ranges = [] # Enter the time ranges IN MINUTES (e.g., 0-10 for 0 to 10 minutes, then 11-30 for remaining 11-30 minutes)\n",
    "\n",
    "for i in range(num_timepoints):\n",
    "    time_range = input(f\"Time range {i + 1}: \")\n",
    "    try:\n",
    "        start_min, end_min = map(int, time_range.split('-'))\n",
    "        if start_min >= end_min:\n",
    "            print(f\"Error: Start time ({start_min}) must be less than end time ({end_min}).\")\n",
    "            exit()\n",
    "        start_sec, end_sec = start_min * 60, end_min * 60\n",
    "        time_ranges.append((start_sec, end_sec))\n",
    "    except ValueError:\n",
    "        print(\"Invalid input format. Please enter the time range as 'start-end' (e.g., 0-10).\")\n",
    "        exit()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9784f94f6e6142b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Individual File Comparisons"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa223f09c0bbda86"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Combined_CSO19_CSO19_1weekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv\n",
      "Bins: [0, 660, 1800]\n",
      "Time Labels: ['0-10 min', '11-30 min']\n",
      "Saved analysis for Combined_CSO19_CSO19_1weekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv to ../processed_dataset/project_ACC_MiniscopeSNI_Male_Psilo_1Week/figures/behavior_timepoint_comparison/analysis_Combined_CSO19_CSO19_1weekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv\n",
      "Processing file: Combined_CSO17_CSO17_1WeekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv\n",
      "Bins: [0, 660, 1800]\n",
      "Time Labels: ['0-10 min', '11-30 min']\n",
      "Saved analysis for Combined_CSO17_CSO17_1WeekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv to ../processed_dataset/project_ACC_MiniscopeSNI_Male_Psilo_1Week/figures/behavior_timepoint_comparison/analysis_Combined_CSO17_CSO17_1WeekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv\n",
      "Warning: Maximum time (842.9833333333333s) in Combined_MM98_MM98_1weekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv is less than the final bin end (1800s).\n",
      "Processing file: Combined_MM98_MM98_1weekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv\n",
      "Bins: [0, 660, 842.9833333333333]\n",
      "Time Labels: ['0-10 min', '11-30 min']\n",
      "Saved analysis for Combined_MM98_MM98_1weekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv to ../processed_dataset/project_ACC_MiniscopeSNI_Male_Psilo_1Week/figures/behavior_timepoint_comparison/analysis_Combined_MM98_MM98_1weekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv\n",
      "Processing file: Combined_CSO15_CSO15_1WeekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv\n",
      "Bins: [0, 660, 1800]\n",
      "Time Labels: ['0-10 min', '11-30 min']\n",
      "Saved analysis for Combined_CSO15_CSO15_1WeekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv to ../processed_dataset/project_ACC_MiniscopeSNI_Male_Psilo_1Week/figures/behavior_timepoint_comparison/analysis_Combined_CSO15_CSO15_1WeekPsilDLC_resnet50_LUPE_MALEDec5shuffle1_350000.csv\n",
      "Behavior analysis completed for all files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define behavior label mapping\n",
    "behavior_map = {\n",
    "    0: 'still',\n",
    "    1: 'walking',\n",
    "    2: 'rearing',\n",
    "    3: 'grooming',\n",
    "    4: 'licking hindpaw L',\n",
    "    5: 'licking hindpaw R'\n",
    "}\n",
    "\n",
    "print(\"time_ranges =\", time_ranges)\n",
    "print(\"len(time_ranges) =\", len(time_ranges))\n",
    "if len(time_ranges) < 2:\n",
    "    print(\"Error: At least two time ranges are required for comparison.\")\n",
    "    exit()\n",
    "\n",
    "bins = [start for start, end in time_ranges] + [time_ranges[-1][1]]\n",
    "time_labels = [f\"{start//60}-{end//60} min\" for start, end in time_ranges]\n",
    "print(\"Using bins:\", bins)\n",
    "print(\"Using time_labels:\", time_labels)\n",
    "\n",
    "input_dir = f'../processed_dataset/{project_name}/figures/behaviors_csv_raw-classification/seconds'\n",
    "print(\"Input directory path (should contain subfolders and CSVs):\", input_dir)\n",
    "\n",
    "analysis_dir = f'../processed_dataset/{project_name}/figures/behavior_timepoint_comparison'\n",
    "print(\"Analysis directory will be:\", analysis_dir)\n",
    "os.makedirs(analysis_dir, exist_ok=True)\n",
    "\n",
    "def calculate_behavior_metrics(data, frame_rate=60):\n",
    "    metrics = {}\n",
    "    unique_behaviors = data['behavior'].unique()\n",
    "    for behavior in unique_behaviors:\n",
    "        behavior_data = data[data['behavior'] == behavior]\n",
    "        fraction_time = len(behavior_data) / len(data)\n",
    "\n",
    "        bout_starts = (behavior_data.index.to_series().diff() > 1).cumsum()\n",
    "        bouts = behavior_data.groupby(bout_starts)\n",
    "        bouts_per_minute = len(bouts) / (len(data) / frame_rate / 60)\n",
    "        mean_bout_duration = bouts.size().mean() / frame_rate\n",
    "\n",
    "        metrics[behavior] = {\n",
    "            'Fraction Time': fraction_time,\n",
    "            'Bouts per Minute': bouts_per_minute,\n",
    "            'Mean Bout Duration (s)': mean_bout_duration\n",
    "        }\n",
    "    return metrics\n",
    "\n",
    "found_any_csv = False\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for file_name in files:\n",
    "        if not file_name.lower().endswith('.csv'):\n",
    "            continue\n",
    "\n",
    "        found_any_csv = True\n",
    "        file_path = os.path.join(root, file_name)\n",
    "        print(\"\\n---\")\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if 'time_seconds' not in df.columns:\n",
    "            print(f\"  ERROR: 'time_seconds' column not found in {file_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        max_time = df['time_seconds'].max()\n",
    "        print(f\"  max_time in this file = {max_time}\")\n",
    "\n",
    "        # Recompute bins if max_time < bins[-1]\n",
    "        bins = [start for start, end in time_ranges] + [time_ranges[-1][1]]\n",
    "        if max_time < bins[-1]:\n",
    "            print(\n",
    "                f\"  WARNING: max_time ({max_time}s) < final bin end ({bins[-1]}s). \"\n",
    "                \"Shrinking last bin to max_time.\"\n",
    "            )\n",
    "            bins[-1] = max_time\n",
    "        print(f\"  Using bins for pd.cut: {bins}\")\n",
    "\n",
    "        try:\n",
    "            df['time_group'] = pd.cut(\n",
    "                df['time_seconds'],\n",
    "                bins=bins,\n",
    "                labels=time_labels,\n",
    "                right=False\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            print(f\"  ValueError in pd.cut for file {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        counts = df['time_group'].value_counts(dropna=False)\n",
    "        print(\"  Row counts per time_group (NaN = didn’t fit any bin):\")\n",
    "        print(counts.to_dict())\n",
    "\n",
    "        all_metrics = []\n",
    "        for time_group, group_data in df.groupby('time_group', observed=False):\n",
    "            if group_data.empty:\n",
    "                continue\n",
    "            metrics = calculate_behavior_metrics(group_data)\n",
    "            for behavior, behavior_metrics in metrics.items():\n",
    "                behavior_name = behavior_map.get(behavior, f\"Unknown ({behavior})\")\n",
    "                all_metrics.append({\n",
    "                    'Time Group': time_group,\n",
    "                    'Behavior': behavior_name,\n",
    "                    **behavior_metrics\n",
    "                })\n",
    "\n",
    "        analysis_df = pd.DataFrame(all_metrics)\n",
    "        print(f\"  Number of metric‐rows to save: {len(analysis_df)}\")\n",
    "\n",
    "        analysis_file_path = os.path.join(\n",
    "            analysis_dir,\n",
    "            f'analysis_{os.path.basename(file_name)}'\n",
    "        )\n",
    "        analysis_df.to_csv(analysis_file_path, index=False)\n",
    "        print(f\"  ⇒ Saved analysis CSV: {analysis_file_path}\")\n",
    "\n",
    "if not found_any_csv:\n",
    "    print(\"No CSV files were found under\", input_dir)\n",
    "\n",
    "print(\"\\nBehavior analysis completed for all files.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-11T18:05:20.351720Z",
     "start_time": "2025-01-11T18:05:20.233750Z"
    }
   },
   "id": "220534d037cacb24",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cohort Comparisons"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4130516229ebd8ff"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cohort summary for group 'Combined' to ../processed_dataset/project_ACC_MiniscopeSNI_Male_Psilo_1Week/figures/behavior_timepoint_comparison/cohort_summaries/Combined_cohort_summary.csv\n",
      "Cohort summaries created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "input_dir = f'../processed_dataset/{project_name}/figures/behavior_timepoint_comparison'\n",
    "cohort_summary_dir = f'../processed_dataset/{project_name}/figures/behavior_timepoint_comparison/cohort_summaries'\n",
    "os.makedirs(cohort_summary_dir, exist_ok=True)\n",
    "\n",
    "behavior_map = {\n",
    "    0: 'still',\n",
    "    1: 'walking',\n",
    "    2: 'rearing',\n",
    "    3: 'grooming',\n",
    "    4: 'licking hindpaw L',\n",
    "    5: 'licking hindpaw R'\n",
    "}\n",
    "\n",
    "def aggregate_cohort_data(condition_name):\n",
    "    all_metrics = []\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith('.csv') and condition_name in file_name:\n",
    "            file_path = os.path.join(input_dir, file_name)\n",
    "            file_data = pd.read_csv(file_path)\n",
    "            all_metrics.append(file_data)\n",
    "\n",
    "    if not all_metrics:\n",
    "        print(f\"No matching files found for condition '{condition_name}'\")\n",
    "        return None\n",
    "\n",
    "    combined_data = pd.concat(all_metrics, ignore_index=True)\n",
    "\n",
    "    summary = combined_data.groupby(['Time Group', 'Behavior']).agg({\n",
    "        'Fraction Time': ['mean', 'std'],\n",
    "        'Bouts per Minute': ['mean', 'std'],\n",
    "        'Mean Bout Duration (s)': ['mean', 'std']\n",
    "    }).reset_index()\n",
    "\n",
    "    summary.columns = ['Time Group', 'Behavior',\n",
    "                       'Fraction Time (mean)', 'Fraction Time (std)',\n",
    "                       'Bouts per Minute (mean)', 'Bouts per Minute (std)',\n",
    "                       'Mean Bout Duration (mean)', 'Mean Bout Duration (std)']\n",
    "\n",
    "    summary = summary.dropna(subset=[\n",
    "        'Fraction Time (mean)',\n",
    "        'Bouts per Minute (mean)',\n",
    "        'Mean Bout Duration (mean)'\n",
    "    ], how='all')\n",
    "\n",
    "    # Map numeric behavior codes to names\n",
    "    summary['Behavior'] = summary['Behavior'].apply(lambda x: behavior_map.get(x, x))\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Loop through each group and each condition\n",
    "for group_name in selected_groups:\n",
    "    for condition_name in selected_conditions:\n",
    "        summary = aggregate_cohort_data(condition_name)\n",
    "        if summary is not None:\n",
    "            output_filename = f'{group_name}_{condition_name}_summary.csv'\n",
    "            summary_file_path = os.path.join(cohort_summary_dir, output_filename)\n",
    "            summary.to_csv(summary_file_path, index=False)\n",
    "            print(f\"Saved cohort summary for group '{group_name}', condition '{condition_name}' to {summary_file_path}\")\n",
    "\n",
    "print(\"Cohort summaries created.\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dir = f'../processed_dataset/{project_name}/figures/behavior_timepoint_comparison'\n",
    "cohort_summary_dir = f'../processed_dataset/{project_name}/figures/behavior_timepoint_comparison/cohort_summaries'\n",
    "os.makedirs(cohort_summary_dir, exist_ok=True)\n",
    "\n",
    "behavior_map = {\n",
    "    0: 'still',\n",
    "    1: 'walking',\n",
    "    2: 'rearing',\n",
    "    3: 'grooming',\n",
    "    4: 'licking hindpaw L',\n",
    "    5: 'licking hindpaw R'\n",
    "}\n",
    "\n",
    "# ——— Data aggregation with SEM ———\n",
    "def aggregate_cohort_data_sem(group_name, condition_name):\n",
    "    \"\"\"Read CSVs matching group+condition, map behaviors, and compute mean, SD, n, SEM.\"\"\"\n",
    "    dfs = []\n",
    "    pattern = f\"{group_name}_{condition_name}\"\n",
    "    for fn in os.listdir(input_dir):\n",
    "        if fn.endswith('.csv') and pattern in fn:\n",
    "            df = pd.read_csv(os.path.join(input_dir, fn))\n",
    "            df['Behavior'] = df['Behavior'].apply(lambda x: behavior_map.get(x, x))\n",
    "            dfs.append(df)\n",
    "    if not dfs:\n",
    "        print(f\"[!] No files for pattern '{pattern}'\")\n",
    "        return None\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # aggregate\n",
    "    agg = combined.groupby(['Time Group','Behavior']).agg({\n",
    "        'Fraction Time':         ['mean','std','count'],\n",
    "        'Bouts per Minute':      ['mean','std','count'],\n",
    "        'Mean Bout Duration (s)': ['mean','std','count']\n",
    "    })\n",
    "    # flatten columns\n",
    "    agg.columns = [\n",
    "        'FT_mean','FT_std','FT_n',\n",
    "        'BPM_mean','BPM_std','BPM_n',\n",
    "        'MBD_mean','MBD_std','MBD_n'\n",
    "    ]\n",
    "    summary = agg.reset_index()\n",
    "\n",
    "    # compute SEMs\n",
    "    summary['FT_sem']  = summary['FT_std']  / np.sqrt(summary['FT_n'])\n",
    "    summary['BPM_sem'] = summary['BPM_std'] / np.sqrt(summary['BPM_n'])\n",
    "    summary['MBD_sem'] = summary['MBD_std'] / np.sqrt(summary['MBD_n'])\n",
    "\n",
    "    return summary\n",
    "\n",
    "# ——— Plotting (with save) ———\n",
    "def plot_metric_side_by_side_sem(summaries, metric_mean, metric_sem, ylabel, title, save_name):\n",
    "    \"\"\"\n",
    "    summaries: dict of {condition_name: summary_df}\n",
    "    metric_mean/metric_sem: column names in summary_df\n",
    "    save_name: filename (no path) to use when saving PNG\n",
    "    \"\"\"\n",
    "    n = len(summaries)\n",
    "    fig, axes = plt.subplots(1, n, figsize=(5*n, 4), sharey=True)\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (cond, df) in zip(axes, summaries.items()):\n",
    "        for beh in df['Behavior'].unique():\n",
    "            d = df[df['Behavior']==beh]\n",
    "            ax.errorbar(\n",
    "                d['Time Group'], \n",
    "                d[metric_mean],\n",
    "                yerr=d[metric_sem],\n",
    "                marker='o',\n",
    "                capsize=3,\n",
    "                label=beh\n",
    "            )\n",
    "        ax.set_title(cond)\n",
    "        ax.set_xlabel('Time Group')\n",
    "        if ax is axes[0]:\n",
    "            ax.set_ylabel(ylabel)\n",
    "        ax.legend(title='Behavior', bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "\n",
    "    fig.suptitle(title, y=1.05)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # --- SAVE FIGURE ---\n",
    "    fig_path = os.path.join(cohort_summary_dir, save_name)\n",
    "    fig.suptitle(title, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    fig.savefig(fig_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved figure: {fig_path}\")\n",
    "\n",
    "# ——— Main loop ———\n",
    "for group in selected_groups:\n",
    "    # aggregate & save CSVs\n",
    "    condition_summaries = {}\n",
    "    for cond in selected_conditions:\n",
    "        summ = aggregate_cohort_data_sem(group, cond)\n",
    "        if summ is None:\n",
    "            continue\n",
    "        # save CSV\n",
    "        csv_name = f'{group}_{cond}_summary.csv'\n",
    "        summ.to_csv(os.path.join(cohort_summary_dir, csv_name), index=False)\n",
    "        print(f\"Saved CSV: {csv_name}\")\n",
    "        condition_summaries[cond] = summ\n",
    "\n",
    "    if not condition_summaries:\n",
    "        continue\n",
    "\n",
    "    # Plot & save each metric\n",
    "    plot_metric_side_by_side_sem(\n",
    "        condition_summaries,\n",
    "        metric_mean='FT_mean', metric_sem='FT_sem',\n",
    "        ylabel='Fraction Time (mean ± SEM)',\n",
    "        title=f'Fraction Time — {group}',\n",
    "        save_name=f'{group}_FractionTime.png'\n",
    "    )\n",
    "    plot_metric_side_by_side_sem(\n",
    "        condition_summaries,\n",
    "        metric_mean='BPM_mean', metric_sem='BPM_sem',\n",
    "        ylabel='Bouts per Min (mean ± SEM)',\n",
    "        title=f'Bouts per Minute — {group}',\n",
    "        save_name=f'{group}_BoutsPerMinute.png'\n",
    "    )\n",
    "    plot_metric_side_by_side_sem(\n",
    "        condition_summaries,\n",
    "        metric_mean='MBD_mean', metric_sem='MBD_sem',\n",
    "        ylabel='Bout Duration (s) (mean ± SEM)',\n",
    "        title=f'Mean Bout Duration — {group}',\n",
    "        save_name=f'{group}_MeanBoutDuration.png'\n",
    "    )\n",
    "\n",
    "print(\"All summaries and figures saved.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-11T18:05:21.140042Z",
     "start_time": "2025-01-11T18:05:21.129740Z"
    }
   },
   "id": "ec0bed5c41605550",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "# COMPLETE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5973ad0d3ed3430e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
